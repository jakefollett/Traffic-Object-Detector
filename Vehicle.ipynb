{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.folder import default_loader\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from torch import optim\n",
    "from tqdm import tqdm, notebook # This is optional but useful\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Traffic(Dataset):\n",
    "  def __init__(self, root_dir, transform=None, loader=default_loader):\n",
    "    \n",
    "    self.image_paths, self.labels = self._get_imgs_and_labels(root_dir)\n",
    "    \n",
    "    self.transform = transform\n",
    "    self.loader = loader\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    image = self.loader(self.image_paths[index])\n",
    "    label = self.labels[index]\n",
    "\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "  \n",
    "    return image, torch.tensor(int(label)-1) # Convert labels to tensors, and subtract 1 for convention purposes (to start at 0)\n",
    "  \n",
    "  # Helper Functions\n",
    "  def print_image(self, index):\n",
    "    img = Image.open(self.image_paths[index])\n",
    "    img.show()\n",
    "    print(self.labels[index])\n",
    "\n",
    "  def _get_imgs_and_labels(self, root_dir):\n",
    "    image_full_path = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get paths to each image (in order)\n",
    "    for img_path in sorted(glob.glob(os.path.join(root_dir, \"images\", \"*.jpg\"))):\n",
    "      image_full_path.append(img_path)\n",
    "    \n",
    "    # Get labels\n",
    "    with open(os.path.join(root_dir, \"annotations.txt\"), 'r') as f:\n",
    "      for label in f:\n",
    "        labels.append(int(label.strip()) + 1)\n",
    "    \n",
    "    return image_full_path, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"Traffic_Processed\"\n",
    "\n",
    "data = Traffic(root_dir=data_dir, transform=T.ToTensor())\n",
    "\n",
    "data.print_image(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional(nn.Module):\n",
    "  def __init__(self, in_channels=3):\n",
    "    super().__init__()\n",
    "    self.conv = nn.Sequential(nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                              \n",
    "                              nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                              \n",
    "                              nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                              \n",
    "                              nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                              \n",
    "                              nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1, stride=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                              \n",
    "                              nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool2d(kernel_size=2, stride=2),                                                \n",
    "    )\n",
    "    self.linear = nn.Sequential(nn.Dropout(p=0.5),\n",
    "                                nn.Linear(in_features=4608, out_features=4608),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(p=0.5),\n",
    "                                nn.Linear(in_features=4608, out_features=7),\n",
    "                                nn.Softmax(dim=1)\n",
    "                                )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.conv(x)\n",
    "    b, c, h, w = x.shape\n",
    "    x = x.view(b, -1) # Flatten image\n",
    "    x = self.linear(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./Traffic_Processed\"\n",
    "\n",
    "transform = T.Compose ([\n",
    "  T.Resize((224, 224)),\n",
    "  T.ToTensor(),\n",
    "  T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "vehicle_data = Traffic(root_dir=data_dir, transform=transform)\n",
    "\n",
    "train_data_size = int(len(vehicle_data) * .1) \n",
    "validation_data_size = int(len(vehicle_data) * .1)\n",
    "other = len(vehicle_data) - (train_data_size + validation_data_size)\n",
    "\n",
    "train_data, val_data, other = random_split(vehicle_data, [train_data_size, validation_data_size, other])\n",
    "\n",
    "'''train = DataLoader(vehicle_data, batch_size=32, shuffle=True)\n",
    "\n",
    "data = iter(train) # Let's iterate on it\n",
    "single_point = next(data)\n",
    "print(f\"\"\"Type: {type(single_point)}\n",
    "Length: {len(single_point)}\n",
    "More Types: {type(single_point[0])}, {type(single_point[1])}\n",
    "Shapes: {single_point[0].shape}, {single_point[1].shape}\n",
    "Labels: {single_point[1]}\n",
    "\"\"\")\n",
    "\n",
    "print(single_point[1][0])\n",
    "print(single_point[1][1])\n",
    "print(single_point[1][2])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms as T\n",
    "ToPIL = T.ToPILImage() # Converting function\n",
    "img0 = ToPIL(single_point[0][0])\n",
    "img1 = ToPIL(single_point[0][1])\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].imshow(img0)\n",
    "axs[1].imshow(img1)'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
